{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "id": "beryQv5c6FTP",
    "ExecuteTime": {
     "end_time": "2024-04-24T13:28:07.552650Z",
     "start_time": "2024-04-24T13:28:07.320029Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Définition d'un line world sous la forme d'un MDP"
   ],
   "metadata": {
    "id": "19mv7NrhVZzx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "psNAv57nOCMW",
    "ExecuteTime": {
     "end_time": "2024-04-24T13:28:12.315439Z",
     "start_time": "2024-04-24T13:28:12.306023Z"
    }
   },
   "outputs": [],
   "source": [
    "S = [0, 1, 2, 3, 4]\n",
    "A = [0, 1]  # left right\n",
    "R = [-1, 0, 1]\n",
    "T = [0, 4]\n",
    "p = np.zeros((len(S), len(A), len(S), len(R)))  # S, A, S_p, R"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for s in range(len(S)):\n",
    "    for a in range(len(A)):\n",
    "        for s_p in range(len(S)):\n",
    "            for r in range(len(R)):\n",
    "                if s_p == s + 1 and a == 1 and R[r] == 0 and s in [1, 2]:\n",
    "                    p[s, a, s_p, r] = 1.0\n",
    "                if s_p == s - 1 and a == 0 and R[r] == 0 and s in [2, 3]:\n",
    "                    p[s, a, s_p, r] = 1.0\n",
    "p[3, 1, 4, 2] = 1.0\n",
    "p[1, 0, 0, 0] = 1.0"
   ],
   "metadata": {
    "id": "tGlcGlTA6Tex",
    "ExecuteTime": {
     "end_time": "2024-04-24T13:28:13.433408Z",
     "start_time": "2024-04-24T13:28:13.425867Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercice"
   ],
   "metadata": {
    "id": "rnLQGzt4Vzxp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implémentez une fonction prenant en paramètre une policy (un tableau numpy)\n",
    "et renvoyant la value de cette policy (un tableau numpy) pour chaque état du line world"
   ],
   "metadata": {
    "id": "s43r9pUuWPtv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def policy_evaluation_on_line_world(policy: np.ndarray) -> np.ndarray:\n",
    "    theta = 0.0001\n",
    "    gamma = 0.999\n",
    "    V = np.random.random(len(S))\n",
    "    V[0] = 0.0\n",
    "    V[4] = 0.0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(len(S)):\n",
    "            v = V[s]\n",
    "            V[s] = sum(policy[s][a] * sum(p[s][a][s_next][r] * (R[r] + gamma * V[s_next]) for s_next in S for r in R) for a in A)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n"
   ],
   "metadata": {
    "id": "_96znS5JWF9H",
    "ExecuteTime": {
     "end_time": "2024-04-24T14:47:19.425553Z",
     "start_time": "2024-04-24T14:47:19.415466Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def naive_ineficient_policy_iteration():\n",
    "    theta = 0.0001\n",
    "    gamma = 0.999\n",
    "    V = np.random.random(len(S))\n",
    "    \n",
    "    for s in T:\n",
    "        V[s] = 0.0\n",
    "    \n",
    "    Pi = np.random.choice(A, len(S))\n",
    "    while True:\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in S:\n",
    "                v = V[s]\n",
    "                V[s] = sum(p[s, Pi[s], s_p, r_index] * (R[r_index] + gamma * V[s_p])for s_p in S for r_index in R)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        policy_stable = True\n",
    "        for s in S:\n",
    "            old_action = Pi[s]\n",
    "            best_a = None\n",
    "            best_action_score = -9999999\n",
    "            \n",
    "            #Pi[s] = #TODO\n",
    "            \n",
    "            "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Définissez une policy jouant tout le temps à droite.\n",
    "Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
   ],
   "metadata": {
    "id": "29NHJWr3WTnW"
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.998001 0.999    1.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "pi_right = [\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "]\n",
    "print(policy_evaluation_on_line_world(pi_right))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:47:21.509156Z",
     "start_time": "2024-04-24T14:47:21.455487Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Définissez une policy jouant tout le temps à gauche. Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
   ],
   "metadata": {
    "id": "imIKxQ_fW1wC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pi_left = None  #TODO\n",
    "print(policy_evaluation_on_line_world(pi_left))"
   ],
   "metadata": {
    "id": "q-_ZCGmRW31Q",
    "ExecuteTime": {
     "end_time": "2024-04-24T14:47:23.949876Z",
     "start_time": "2024-04-24T14:47:23.863507Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m pi_left \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m#TODO\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mpolicy_evaluation_on_line_world\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpi_left\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[7], line 12\u001B[0m, in \u001B[0;36mpolicy_evaluation_on_line_world\u001B[0;34m(policy)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(S)):\n\u001B[1;32m     11\u001B[0m     v \u001B[38;5;241m=\u001B[39m V[s]\n\u001B[0;32m---> 12\u001B[0m     V[s] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms_next\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mR\u001B[49m\u001B[43m[\u001B[49m\u001B[43mr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mV\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms_next\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms_next\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mS\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mR\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mA\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(delta, \u001B[38;5;28mabs\u001B[39m(v \u001B[38;5;241m-\u001B[39m V[s]))\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m delta \u001B[38;5;241m<\u001B[39m theta:\n",
      "Cell \u001B[0;32mIn[7], line 12\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(S)):\n\u001B[1;32m     11\u001B[0m     v \u001B[38;5;241m=\u001B[39m V[s]\n\u001B[0;32m---> 12\u001B[0m     V[s] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[43mpolicy\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m[a] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28msum\u001B[39m(p[s][a][s_next][r] \u001B[38;5;241m*\u001B[39m (R[r] \u001B[38;5;241m+\u001B[39m gamma \u001B[38;5;241m*\u001B[39m V[s_next]) \u001B[38;5;28;01mfor\u001B[39;00m s_next \u001B[38;5;129;01min\u001B[39;00m S \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m R) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m A)\n\u001B[1;32m     13\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(delta, \u001B[38;5;28mabs\u001B[39m(v \u001B[38;5;241m-\u001B[39m V[s]))\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m delta \u001B[38;5;241m<\u001B[39m theta:\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Définissez une policy uniformément aléatoire (50% de chance d'aller à gauche et 50% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
   ],
   "metadata": {
    "id": "N_8nQtoBXJai"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pi_random = None  #TODO\n",
    "print(policy_evaluation_on_line_world(pi_random))"
   ],
   "metadata": {
    "id": "eUYtT2hYW_Fe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Définissez une policy uniformément aléatoire (15% de chance d'aller à gauche et 85% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
   ],
   "metadata": {
    "id": "XQpwh-x_XXGN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pi_weird_random = None  #TODO\n",
    "print(policy_evaluation_on_line_world(pi_weird_random))"
   ],
   "metadata": {
    "id": "h-HkR7HCXc2b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Définissez une variante d'un GRID WORLD sous la forme d'un MDP et évaluez différentes stratégies sur cette environnement.\n",
    "\n",
    "Le grid world est une grille de 5x5 cases (5 lignes de 5 colonnes) sur laquelle l'agent peut évoluer, il commence généralement sur la première ligne, première colonne. L'agent possède 4 actions possibles (gauche, droite, haut, bas). Si jamais l'agent atteint la dernière case de la première ligne => état terminal avec reward de -3. Si jamais l'agent atteint la dernière case de la dernière ligne => état terminal avec reward de 1. Si l'agent essaye de se déplacer en dehors des bords de la grille => état terminal avec score de -1."
   ],
   "metadata": {
    "id": "EZshaI9mXiaF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "S = None  #TODO\n",
    "A = None  #TODO\n",
    "R = None  #TODO\n",
    "p = None  #TODO"
   ],
   "metadata": {
    "id": "l_eC0-d9Xh7Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proposez plusieurs stratégies et évaluez les à l'aide de policy évaluation."
   ],
   "metadata": {
    "id": "YdZTA1y6YwkJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO"
   ],
   "metadata": {
    "id": "Ve8dvoONYuq5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus : Implémentez policy itération et value itération (dans les slides que nous n'avons pas encore vu) et obtenez à l'aide de ces derniers pi* pour le Line World et le Grid World"
   ],
   "metadata": {
    "id": "ex0zmlgSY4UR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO"
   ],
   "metadata": {
    "id": "J-vUDJ_0ZJaO"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
